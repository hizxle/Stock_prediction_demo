# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WGMVxum0gQBLRjS5wVyOqgBNQ1hXp8H0

# Импорт датасетов

Через Google Drive
"""

news = pd.read_csv('/content/drive/MyDrive/news_processed.csv')
df = pd.read_csv('/content/drive/MyDrive/train.csv')

"""Напрямую"""

news = pd.read_csv('/content/news_processed.csv')
df = pd.read_csv('/content/train.csv')

"""# Импорт библиотек"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import tensorflow as tf
import seaborn as sns

"""# Построение эмбеддингов"""

from transformers import pipeline
pipe = pipeline("feature-extraction", model="DeepPavlov/rubert-base-cased",)

# Load model directly
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")
model = AutoModel.from_pretrained("DeepPavlov/rubert-base-cased")

def get_embs(batch):
  encoded_input = tokenizer(batch, return_tensors='pt', padding="longest", truncation=True, max_length=512)

  # Generate embeddings
  with torch.no_grad():
      outputs = model(**encoded_input)

  # Get the last hidden state of the model, which represents the embeddings
  embeddings = outputs.last_hidden_state

  # Average the embeddings to get a single sentence embedding
  sentence_embedding = torch.mean(embeddings, dim=1).squeeze()
  return sentence_embedding

def get_embeddings(text_series, batch_size=32):
    text_series = text_series.astype(str)
    embeddings_list = []
    for i in range(0, len(text_series), batch_size):
        batch = text_series[i:i+batch_size].to_list()
        embeddings = get_embs(batch)
        embeddings_list.append(embeddings)
    return torch.concat(embeddings_list, axis=0)

news_embeddings_list = [get_embeddings(news[col]) for col in ['title', 'content']]

pd.DataFrame(news_embeddings_list[0]).to_csv('news_embedding_title.csv')
pd.DataFrame(news_embeddings_list[1]).to_csv('news_embedding_content.csv')

title = news_embeddings_list[0]
content = news_embeddings_list[1]

"""# Предобработка данных

Feature Engineering
"""

df['datetime'] = pd.to_datetime(df['DATE']+' '+df['TIME'])

df = df.set_index(['TICKER', 'datetime'])

news['datetime'] = pd.to_datetime(news['datetime'])

df['datetime'] = pd.to_datetime(df['DATE']+' '+df['TIME'])
df = df.drop(['TIME','DATE','PER'], axis=1)

"""Преобразования датасетов"""

df_outer_join = pd.merge(news, content, on='Unnamed: 0', how='outer')

news_with_embeddings = pd.merge(df_outer_join, titles, on='Unnamed: 0', how = 'outer')

dataset = []
for i, row in news_with_embeddings.iterrows():
  for tick in ('SBER', ):
    try:

      date = row['datetime']
      date -= dt.timedelta(seconds = date.second)
      td = dt.timedelta(hours=1)
      delta = df.loc[(tick, pd.Timestamp(date))] - df.loc[(tick, pd.Timestamp(date - td))]
      el_to_append = {
          'title': row['title'],
          'tick': tick,
          'score': delta['CLOSE'],
          'content': row['content'],
          'datetime':row['datetime'],
          'open':df.loc[(tick, pd.Timestamp(date))]['OPEN'],
          'close':df.loc[(tick, pd.Timestamp(date+td))]['CLOSE'],
          'low': df.loc[(tick, pd.Timestamp(date))]['LOW'],
          'high': df.loc[(tick, pd.Timestamp(date))]['HIGH'],
          'vol': df.loc[(tick, pd.Timestamp(date))]['VOL']

      }
      for i in range(768):
        el_to_append[f'{i}_x'] = row[f'{i}_x']
        el_to_append[f'{i}_y'] = row[f'{i}_y']
      dataset.append(el_to_append)
    except:
      pass

df = pd.DataFrame(dataset)

df = df.drop(['content', 'title'], axis = 1)

"""Экспорт итогового датасета"""

ready_df.to_csv('dataframe_no_text.csv', index = False)